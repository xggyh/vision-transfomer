{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vit 实现的第一种方法，也是比赛中使用的就是从timm库里面直接调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name='vit_base_patch16_224', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cnn = timm.create_model(model_name, pretrained)\n",
    "        self.n_features = self.cnn.head.in_features\n",
    "        self.cnn.head = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.cnn.patch_embed(x)\n",
    "        # 我们的vit有个cls头\n",
    "        cls_token = self.cnn.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.cnn.pos_embed\n",
    "        x = self.cnn.pos_drop(x)\n",
    "        for blk in self.cnn.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.cnn.norm(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Encoder()\n",
    "a = torch.rand(4,3,224,224)\n",
    "model(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 768])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i,j in a.named_parameters():\n",
    "#     print(i, j.size())\n",
    "model.cnn.patch_embed(a).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vit 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "patch_size = 16\n",
    "d_model = 768\n",
    "n_head = 8\n",
    "qkv_dim = 96 # 在vit中qkv_dim等于d_model/n_head\n",
    "n_layers = 3\n",
    "ffn_dim = 768\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有mask的transfomer结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, image_dim=d_model, head=n_head, dim_head=qkv_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * head\n",
    "        self.image_dim = image_dim\n",
    "        self.head = head\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.to_qkv = nn.Linear(image_dim, inner_dim*3, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, image_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x, mask=None):\n",
    "        '''x_size : bs, leng((image_size/patch_size)**2+1), dim'''\n",
    "        b, n, _, = x.shape\n",
    "        residual = x\n",
    "        head = self.head\n",
    "        q,k,v = self.to_qkv(x).chunk(3, dim=-1) # bs, len, dim_head*n_head\n",
    "        q = q.reshape(b, n, head, -1).transpose(1,2) # bs, len, dim_head*n_head->bs, len, n_head, dim_head->bs, n_head, len, dim_head\n",
    "        k = k.reshape(b, n, head, -1).transpose(1,2)\n",
    "        v = v.reshape(b, n, head, -1).transpose(1,2)\n",
    "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / self.scale \n",
    "        attn_score = nn.Softmax(dim=-1)(attn_score)\n",
    "        \n",
    "        context = torch.matmul(attn_score, v)\n",
    "        context = context.transpose(1, 2).reshape(b, n, -1)\n",
    "        output = self.to_out(context)\n",
    "        return nn.LayerNorm(self.image_dim)(residual+output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim=d_model, hidden_dim=ffn_dim, dropout=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = self.net(x)\n",
    "        return nn.LayerNorm(self.dim)(residual+output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfomer(nn.Module):\n",
    "    def __init__(self, dim=d_model, depth=n_layers):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for i in range(depth):\n",
    "            self.layers.append(Attention())\n",
    "            self.layers.append(FeedForward())\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transfomer()\n",
    "model(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size=image_size, patch_size=patch_size, num_classes=num_classes, dim=d_model, depth=n_layers, heads=n_head, mlp_dim=ffn_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size//patch_size)**2\n",
    "        self.conv = nn.Conv2d(3, dim, patch_size, stride=patch_size, padding=patch_size//2-1)\n",
    "        self.patch_size = patch_size\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches+1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.transfomer = Transfomer()\n",
    "        self.pool = pool\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))\n",
    "    def forward(self, img):\n",
    "        p = self.num_patches\n",
    "        b = img.size(0)\n",
    "        x = self.conv(img).permute(0,2,3,1).contiguous().reshape(b, p, -1)\n",
    "        _,n,_ = x.size()\n",
    "        cls_token = self.cls_token.repeat(b, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x += self.pos_embed\n",
    "        x = self.transfomer(x)\n",
    "        x = x.mean(dim=1) if self.pool=='mean' else x[:, 0]\n",
    "        return self.mlp_head(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, d_model, patch_size, stride=patch_size, padding=patch_size//2-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 196, 768])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(4,3,224,224)\n",
    "conv(img).permute(0,2,3,1).contiguous().reshape(4, (image_size//patch_size)**2, -1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(4,3,224,224)\n",
    "model = ViT()\n",
    "model(img).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 197, 768])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit55be755885774eb0a7ad3a0e48bf7753"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
